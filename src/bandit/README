Multi-armed bandit code for determining the minimum or maximum of "bandits"
that return either 0 or 1 according to an unknown probability.

The point is to find, among other things, the method that is most resistant
to strategy. Each pull of the bandit corresponds to generating a random ballot
profile and then trying to find a strategy that gives the win to someone the
strategists prefer; and the bandit returns either susceptible or not, according
to an unknown mean that depends on the method.

I currently use lil'UCB. Track-and-stop might still be better, see
https://github.com/jsfunc/best-arm-identification
https://arxiv.org/pdf/1702.00001.pdf
http://proceedings.mlr.press/v49/garivier16a.pdf

but I think the margin has shrunk considerably compared to when I used
lUCB. Next thing is probably going to be Pareto frontier identification rather
than further improvements on best-arm identification.

Note that the bandit model I'm using is exploration only, because we're 
interested in finding the best performer, not to create a metamethod that
generally performs well by a chosen metric (like strategy resistance).
